{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1 - DrQA, Document reader Question Answering.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNbESkBXxHvfxvShB4a49KZ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dksifoua/Question-Answering/blob/master/1%20-%20DrQA%2C%20Document%20reader%20Question%20Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LMGpLvYNKR78",
    "outputId": "a44642e8-e103-4b7c-eb0f-625a56ebe146",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  5 01:51:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.01       Driver Version: 516.01       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:08:00.0  On |                  N/A |\n",
      "|  0%   39C    P8    15W / 170W |    583MiB / 12288MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       276    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6520    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6648    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      8088    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8172    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8456    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9572    C+G   ...1\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    0   N/A  N/A     10256    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     11528    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     12532    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     14268    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     14888    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     14960    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     15756    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     18424    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     19316    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP-9tyjdylTg"
   },
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bm6QVw_hzHB7"
   },
   "source": [
    "# !pip install tqdm --upgrade >> /dev/null 2>&1\n",
    "# !pip install spacy --upgrade >> /dev/null 2>&1\n",
    "# !python -m spacy download en >> /dev/null 2>&1"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJQv4DSelwnB"
   },
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import collections\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.tokens import Doc\n",
    "from typing import Dict, List, NamedTuple\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cw16-fZCDHHu",
    "outputId": "29d412de-e92b-46dc-d61e-f679089028fc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "SEED = 546\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx9pL3Hiyn7c"
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "***Download data***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kBy3v4Pke7dq",
    "outputId": "4d0d9d8a-d05a-4d3f-b1dd-545aa92ad6f1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# !rm -rf ./data\n",
    "# !mkdir ./data\n",
    "#\n",
    "# !wget --no-check-certificate \\\n",
    "#     https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json \\\n",
    "#     -O ./data/train-v1.1.json\n",
    "#\n",
    "# !wget --no-check-certificate \\\n",
    "#     https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json \\\n",
    "#     -O ./data/dev-v1.1.json"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9H59xUnyvAR"
   },
   "source": [
    "***Load JSON data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Target(NamedTuple):\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "\n",
    "\n",
    "class TokenFeature(NamedTuple):\n",
    "    exact_match: List[bool]\n",
    "    part_of_speech: List[str]\n",
    "    named_entity_type: List[str]\n",
    "    normalized_term_frequency: List[float]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SquadV1DataItem:\n",
    "    id_: str\n",
    "    context: Doc\n",
    "    question: Doc\n",
    "    answer: Doc\n",
    "    answer_start_index: int\n",
    "    target: Target = None\n",
    "    token_feature: TokenFeature = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IbZQjhA5roqo"
   },
   "source": [
    "def load_squad_v1_data(path: str) -> Dict:\n",
    "    try:\n",
    "        json_file = open(path, mode='r', encoding=\"utf-8\")\n",
    "        return json.load(json_file)\n",
    "    except IOError:\n",
    "        raise IOError"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rqjrkTxHr3rA",
    "outputId": "aeed0ca6-a730-499b-cd3c-8f2bed254f81",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "train_raw_data = load_squad_v1_data(path=\"../data/train-v1.1.json\")\n",
    "valid_raw_data = load_squad_v1_data(path=\"../data/dev-v1.1.json\")\n",
    "print(f\"Length of raw train data: {len(train_raw_data['data']):,}\")\n",
    "print(f\"Length of raw valid data: {len(valid_raw_data['data']):,}\")"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw train data: 442\n",
      "Length of raw valid data: 48\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4sypWlVyxx-"
   },
   "source": [
    "***Parse JSON data***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O-6GE3YxsQsl"
   },
   "source": [
    "def parse_squad_v1_data(data: Dict, spacy_nlp: spacy.language.Language) -> List[SquadV1DataItem]:\n",
    "    qas = []\n",
    "    disabled_components = [\"parser\", \"lemmatizer\", \"tagger\", \"ner\"]\n",
    "    for paragraphs in tqdm.tqdm(data[\"data\"]):\n",
    "        for paragraph in paragraphs[\"paragraphs\"]:\n",
    "            context = spacy_nlp(paragraph[\"context\"], disable=disabled_components[:1])\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                id_ = qa[\"id\"]\n",
    "                question = spacy_nlp(qa[\"question\"], disable=disabled_components)\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    qas.append(\n",
    "                        SquadV1DataItem(id_=id_, context=context, question=question,\n",
    "                                        answer=spacy_nlp(answer[\"text\"], disable=disabled_components),\n",
    "                                        answer_start_index=answer[\"answer_start\"])\n",
    "                    )\n",
    "    return qas"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6MeqBr7Ov9ZU",
    "outputId": "2dad6d53-186e-41d5-dd61-0da6ac26c314",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "train_qas = parse_squad_v1_data(data=train_raw_data, spacy_nlp=nlp)\n",
    "valid_qas = parse_squad_v1_data(data=valid_raw_data, spacy_nlp=nlp)\n",
    "print()\n",
    "print(f\"Length of train qa pairs: {len(train_qas):,}\")\n",
    "print(f\"Length of valid qa pairs: {len(valid_qas):,}\")\n",
    "print(\"==================== Train example ====================\")\n",
    "index = np.random.randint(len(train_qas))\n",
    "print(\"Id:\", train_qas[index].id_)\n",
    "print(\"Context:\", train_qas[index].context)\n",
    "print(\"Question:\", train_qas[index].context)\n",
    "print(\"Answer starts at:\", train_qas[index].answer_start_index)\n",
    "print(\"Answer:\", train_qas[index].answer)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [13:56<00:00,  1.89s/it]\n",
      "100%|██████████| 48/48 [03:37<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of train qa pairs: 87,599\n",
      "Length of valid qa pairs: 34,726\n",
      "==================== Train example ====================\n",
      "Id: 5709e9336d058f1900182c1c\n",
      "Context: According to FIG rules, only women compete in rhythmic gymnastics. This is a sport that combines elements of ballet, gymnastics, dance, and apparatus manipulation. The sport involves the performance of five separate routines with the use of five apparatus; ball, ribbon, hoop, clubs, rope—on a floor area, with a much greater emphasis on the aesthetic rather than the acrobatic. There are also group routines consisting of 5 gymnasts and 5 apparatuses of their choice. Rhythmic routines are scored out of a possible 30 points; the score for artistry (choreography and music) is averaged with the score for difficulty of the moves and then added to the score for execution.\n",
      "Question: According to FIG rules, only women compete in rhythmic gymnastics. This is a sport that combines elements of ballet, gymnastics, dance, and apparatus manipulation. The sport involves the performance of five separate routines with the use of five apparatus; ball, ribbon, hoop, clubs, rope—on a floor area, with a much greater emphasis on the aesthetic rather than the acrobatic. There are also group routines consisting of 5 gymnasts and 5 apparatuses of their choice. Rhythmic routines are scored out of a possible 30 points; the score for artistry (choreography and music) is averaged with the score for difficulty of the moves and then added to the score for execution.\n",
      "Answer starts at: 516\n",
      "Answer: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bafKgiFv1GHa"
   },
   "source": [
    "def test_answer_start_indexes(qas: List[SquadV1DataItem]) -> None:\n",
    "    for qa in tqdm.tqdm(qas):  # type: SquadV1DataItem\n",
    "        assert qa.answer.text == qa.context.text[qa.answer_start_index:qa.answer_start_index + len(qa.answer.text)]"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3tTDTQod23-g",
    "outputId": "022c7d5f-cfbf-4b12-b4f9-e6882a9cef07",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "test_answer_start_indexes(qas=train_qas)\n",
    "test_answer_start_indexes(qas=valid_qas)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:05<00:00, 15325.05it/s]\n",
      "100%|██████████| 34726/34726 [00:02<00:00, 15699.80it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxhCfiNY28zs"
   },
   "source": [
    "***Add targets***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k4bpqV3S27SQ"
   },
   "source": [
    "def add_targets_to_squad_v1_data(qas: List[SquadV1DataItem]) -> None:\n",
    "    for qa in tqdm.tqdm(qas):  # type: SquadV1DataItem\n",
    "        for i in range(len(qa.context)):\n",
    "            if qa.context[i].idx == qa.answer_start_index:\n",
    "                answer = qa.context[i:i + len(qa.answer)]\n",
    "                qa.target = Target(start_index=answer[0].i, end_index=answer[-1].i)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "waJedA674CCH",
    "outputId": "84672e5a-a68e-473b-8be8-c7a2e10a0698",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "add_targets_to_squad_v1_data(qas=train_qas)\n",
    "add_targets_to_squad_v1_data(qas=valid_qas)\n",
    "print(f\"Length of train qa pairs: {len(train_qas):,}\")\n",
    "print(f\"Length of valid qa pairs: {len(valid_qas):,}\")"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:02<00:00, 32791.89it/s]\n",
      "100%|██████████| 34726/34726 [00:01<00:00, 31954.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train qa pairs: 87,599\n",
      "Length of valid qa pairs: 34,726\n",
      "Wall time: 3.78 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lT65eePm4F9w"
   },
   "source": [
    "def is_bad_item(qa: SquadV1DataItem) -> bool:\n",
    "    \"\"\"Return True if either the target is None or target indexes don't match the answer. Return False otherwise\"\"\"\n",
    "    if qa.target is None:\n",
    "        return False\n",
    "    return qa.answer.text == qa.context[qa.target.start_index:qa.target.end_index + 1].text"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V_nUlM_n4Mgc",
    "outputId": "cde0cb35-dd18-4ba5-f4a4-a63a46ec6db7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "train_qas = [*filter(is_bad_item, train_qas)]\n",
    "valid_qas = [*filter(is_bad_item, valid_qas)]\n",
    "print(f\"Length of train qa pairs after filtering out bad qa pairs: {len(train_qas):,}\")\n",
    "print(f\"Length of valid qa pairs after filtering out bad qa pairs: {len(valid_qas):,}\")"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train qa pairs after filtering out bad qa pairs: 86,676\n",
      "Length of valid qa pairs after filtering out bad qa pairs: 34,362\n",
      "Wall time: 882 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qKEhn4eI4P5B"
   },
   "source": [
    "def test_targets(qas: List[SquadV1DataItem]) -> None:\n",
    "    for qa in qas:\n",
    "        assert qa.answer.text == qa.context[qa.target.start_index:qa.target.end_index + 1].text"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6O_fmDJl5R7V",
    "outputId": "999e5ada-e4ec-44ae-f40c-682b08cbb7b6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "test_targets(qas=train_qas)\n",
    "test_targets(qas=valid_qas)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 834 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkLppZOP5Vju"
   },
   "source": [
    "***Add features***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O8Y038pt5VIc"
   },
   "source": [
    "def add_extra_features_squad_v1(qas: List[SquadV1DataItem]) -> None:\n",
    "    \"\"\"Add extra features: Exact Match, Part-of-Speech, Name Entity Recognition & Normalized Term Frequency\"\"\"\n",
    "    for qa in tqdm.tqdm(qas):  # type: SquadV1DataItem\n",
    "        question = [token.text.lower() for token in qa.question]\n",
    "        count_context_tokens = collections.Counter(map(lambda token: token.text.lower(), qa.context))\n",
    "\n",
    "        frequency_context_tokens: Dict[int, int] = {}\n",
    "        for index, token in enumerate(qa.context):  # type: int, Token\n",
    "            frequency_context_tokens[index] = count_context_tokens[token.text.lower()]\n",
    "        norm_frequency_context_tokens = sum(frequency_context_tokens.values())\n",
    "\n",
    "        qa.token_feature = TokenFeature(\n",
    "            exact_match=[qa.context[index].text.lower() in question for index in range(len(qa.context))],\n",
    "            part_of_speech=[qa.context[index].tag_ for index in range(len(qa.context))],\n",
    "            named_entity_type=[qa.context[index].ent_type_ or \"None\" for index in range(len(qa.context))],\n",
    "            normalized_term_frequency=[\n",
    "                frequency_context_tokens[index] / norm_frequency_context_tokens for index in range(len(qa.context))\n",
    "            ]\n",
    "        )"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jqwcbDgPSmp8",
    "outputId": "8df2a4a8-4c18-478b-980e-16ab98a78e3a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "add_extra_features_squad_v1(qas=train_qas)\n",
    "add_extra_features_squad_v1(qas=valid_qas)"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86676/86676 [00:32<00:00, 2670.18it/s]\n",
      "100%|██████████| 34362/34362 [00:14<00:00, 2432.23it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8SrKXlfoU5y"
   },
   "source": [
    "***Build vocabularies***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2LYjZhu7m6YI"
   },
   "source": [
    "class Vocab:\n",
    "\n",
    "    def __init__(self, pad_token, unk_token):\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.vocab = None\n",
    "        self.word2count = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "    \n",
    "    def build(self, data, min_freq):\n",
    "        \"\"\"\n",
    "        :param List[Union[spacy.tokens.doc.Doc, str, Tuple]] data\n",
    "        :param int min_freq\n",
    "        \"\"\"\n",
    "        words = [self.pad_token, self.unk_token]\n",
    "        type_0 = type(data[0])\n",
    "        if type_0 == spacy.tokens.doc.Doc:\n",
    "            for item in data: # context and question\n",
    "                words += [word.text.lower() for word in item]\n",
    "        elif type_0 == str: # id\n",
    "            words += data\n",
    "        elif type_0 == tuple: # pos and ner\n",
    "            for item in data:\n",
    "                words += [word.lower() for word in item]\n",
    "        self.word2count = collections.Counter(words)\n",
    "        self.vocab = sorted(filter(\n",
    "            lambda word: self.word2count[word] >= min_freq or word == self.pad_token or word == self.unk_token, self.word2count\n",
    "        ))\n",
    "        self.word2index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index2word = {index: word for index, word in enumerate(self.vocab)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def stoi(self, word):\n",
    "        return self.word2index.get(str(word), self.word2index[self.unk_token])\n",
    "\n",
    "    def itos(self, index):\n",
    "        return self.index2word[index]"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2RP28pUqYoQ",
    "outputId": "e802e003-1163-48f5-a3c4-d66077b0e883",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "ID = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
    "POS = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
    "NER = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
    "TEXT = Vocab(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN)\n",
    "\n",
    "ids = [*map(lambda qa: qa['id'], train_qas)] + [*map(lambda qa: qa['id'], valid_qas)]\n",
    "pos, ner, contexts, questions = zip(*map(lambda qa: (qa['pos'], qa['ner'], qa['context'], qa['question']), train_qas))\n",
    "\n",
    "ID.build(data=[*set(ids)], min_freq=0)\n",
    "POS.build(data=[*set(pos)], min_freq=0)\n",
    "NER.build(data=[*set(ner)], min_freq=0)\n",
    "TEXT.build(data=[*set(contexts)] + [*set(questions)], min_freq=5)\n",
    "\n",
    "print(f'Length of ID vocabulary: {len(ID):,}')\n",
    "print(f'Length of POS vocabulary: {len(POS):,}')\n",
    "print(f'Length of NER vocabulary: {len(NER):,}')\n",
    "print(f'Length of TEXT vocabulary: {len(TEXT):,}')"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Length of ID vocabulary: 97,108\n",
      "Length of POS vocabulary: 52\n",
      "Length of NER vocabulary: 21\n",
      "Length of TEXT vocabulary: 26,885\n",
      "CPU times: user 5.92 s, sys: 268 ms, total: 6.19 s\n",
      "Wall time: 6.19 s\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t26xjG9OAikb"
   },
   "source": [
    "***Build datasets***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iotg02zlAflQ"
   },
   "source": [
    "class SQuADV1Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, id_vocab, pos_vocab, ner_vocab, text_vocab):\n",
    "        self.data = data\n",
    "        self.id_vocab = id_vocab\n",
    "        self.pos_vocab = pos_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.text_vocab = text_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        id = torch.LongTensor([self.id_vocab.stoi(item['id'])])\n",
    "        ctx = torch.LongTensor([*map(lambda token: self.text_vocab.stoi(token.text.lower()), item['context'])])\n",
    "        qst = torch.LongTensor([*map(lambda token: self.text_vocab.stoi(token.text.lower()), item['question'])])\n",
    "        trg = torch.LongTensor(item['target'])\n",
    "        em = torch.LongTensor(item['em'])\n",
    "        pos = torch.LongTensor([*map(lambda token: self.pos_vocab.stoi(token.lower()), item['pos'])])\n",
    "        ner = torch.LongTensor([*map(lambda token: self.ner_vocab.stoi(token.lower()), item['ner'])])\n",
    "        ntf = torch.FloatTensor(item['ntf'])\n",
    "        return id, ctx, qst, trg, em, pos, ner, ntf"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XMgSisEt8ARe",
    "outputId": "735861e8-f6c2-49bb-b8e8-7d5da12ef11a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "train_dataset = SQuADV1Dataset(data=train_qas, id_vocab=ID, pos_vocab=POS, ner_vocab=NER, text_vocab=TEXT)\n",
    "valid_dataset = SQuADV1Dataset(data=valid_qas, id_vocab=ID, pos_vocab=POS, ner_vocab=NER, text_vocab=TEXT)\n",
    "\n",
    "id, ctx, qst, trg, em, pos, ner, ntf = train_dataset[0]\n",
    "print(f'id shape: {id.shape}')\n",
    "print(f'ctx shape: {ctx.shape}')\n",
    "print(f'qst shape: {qst.shape}')\n",
    "print(f'trg shape: {trg.shape}')\n",
    "print(f'em shape: {em.shape}')\n",
    "print(f'pos shape: {pos.shape}')\n",
    "print(f'ner shape: {ner.shape}')\n",
    "print(f'ntf shape: {ntf.shape}')"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "id shape: torch.Size([1])\n",
      "ctx shape: torch.Size([142])\n",
      "qst shape: torch.Size([14])\n",
      "trg shape: torch.Size([2])\n",
      "em shape: torch.Size([142])\n",
      "pos shape: torch.Size([142])\n",
      "ner shape: torch.Size([142])\n",
      "ntf shape: torch.Size([142])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZQaVaHt_MDc"
   },
   "source": [
    "***Build data loaders***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PPKrJOpKTlN1"
   },
   "source": [
    "class DotDict(dict):\n",
    "    __getattr__ = dict.get"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cYMYz3-SBYzf"
   },
   "source": [
    "def add_padding(batch, pad_token=PAD_TOKEN, text_vocab=TEXT, pos_vocab=POS, ner_vocab=NER, include_lengths=True, device=DEVICE):\n",
    "    \"\"\"Pad batch of sequence with different lengths\"\"\"\n",
    "    batch_id, batch_ctx, batch_qst, batch_trg, batch_em, batch_pos, batch_ner, batch_ntf = zip(*batch)\n",
    "    if include_lengths:\n",
    "        len_ctx = torch.LongTensor([ctx.size(0) for ctx in batch_ctx]).to(device)\n",
    "        len_qst = torch.LongTensor([qst.size(0) for qst in batch_qst]).to(device)\n",
    "    batch_padded_id = pad_sequence(batch_id, batch_first=True).to(device)\n",
    "    batch_padded_ctx = pad_sequence(batch_ctx, batch_first=True, padding_value=text_vocab.stoi(pad_token)).to(device)\n",
    "    batch_padded_qst = pad_sequence(batch_qst, batch_first=True, padding_value=text_vocab.stoi(pad_token)).to(device)\n",
    "    batch_padded_trg = pad_sequence(batch_trg, batch_first=True).to(device)\n",
    "    batch_padded_em = pad_sequence(batch_em, batch_first=True).to(device)\n",
    "    batch_padded_pos = pad_sequence(batch_pos, batch_first=True, padding_value=pos_vocab.stoi(pad_token)).to(device)\n",
    "    batch_padded_ner = pad_sequence(batch_ner, batch_first=True, padding_value=ner_vocab.stoi(pad_token)).to(device)\n",
    "    batch_padded_ntf = pad_sequence(batch_ntf, batch_first=True).to(device)\n",
    "    return DotDict({\n",
    "        'id': batch_padded_id,\n",
    "        'ctx': (batch_padded_ctx, len_ctx) if include_lengths else batch_padded_ctx,\n",
    "        'qst': (batch_padded_qst, len_qst) if include_lengths else batch_padded_qst,\n",
    "        'trg': batch_padded_trg,\n",
    "        'em': batch_padded_em,\n",
    "        'pos': batch_padded_pos,\n",
    "        'ner': batch_padded_ner,\n",
    "        'ntf': batch_padded_ntf,\n",
    "    })"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zu2ovPqxA7Ix",
    "outputId": "02a52daa-a8ec-45de-c916-995fe7e860c4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=add_padding)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print('batch.id shape:', batch.id.shape)\n",
    "    print('batch.ctx shape:', batch.ctx[0].shape, batch.ctx[1].shape)\n",
    "    print('batch.qst shape:', batch.qst[0].shape, batch.qst[1].shape)\n",
    "    print('batch.trg shape:', batch.trg.shape)\n",
    "    print('batch.em shape:', batch.em.shape)\n",
    "    print('batch.pos shape:', batch.pos.shape)\n",
    "    print('batch.ner shape:', batch.ner.shape)\n",
    "    print('batch.ntf shape:', batch.ntf.shape)\n",
    "    break"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "batch.id shape: torch.Size([32, 1])\n",
      "batch.ctx shape: torch.Size([32, 253]) torch.Size([32])\n",
      "batch.qst shape: torch.Size([32, 19]) torch.Size([32])\n",
      "batch.trg shape: torch.Size([32, 2])\n",
      "batch.em shape: torch.Size([32, 253])\n",
      "batch.pos shape: torch.Size([32, 253])\n",
      "batch.ner shape: torch.Size([32, 253])\n",
      "batch.ntf shape: torch.Size([32, 253])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2O79r_CLvGu"
   },
   "source": [
    "***Download pretrained GloVe embedding***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hrSXYalOmRak",
    "outputId": "bc303808-01ad-4f75-c884-0a337681eb7b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "!wget --no-check-certificate \\\n",
    "    http://nlp.stanford.edu/data/glove.840B.300d.zip \\\n",
    "    -O ./data/glove.840B.300d.zip\n",
    "!unzip -q ./data/glove.840B.300d.zip -d ./data\n",
    "!rm -r ./data/glove.840B.300d.zip"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2020-11-01 16:23:56--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
      "--2020-11-01 16:23:57--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
      "--2020-11-01 16:23:57--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176768927 (2.0G) [application/zip]\n",
      "Saving to: ‘./data/glove.840B.300d.zip’\n",
      "\n",
      "./data/glove.840B.3 100%[===================>]   2.03G  2.19MB/s    in 16m 54s \n",
      "\n",
      "2020-11-01 16:40:51 (2.05 MB/s) - ‘./data/glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
      "\n",
      "CPU times: user 2.93 s, sys: 1.02 s, total: 3.95 s\n",
      "Wall time: 18min 48s\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BL0JZuEmmf5C"
   },
   "source": [
    "def load_glove(path):\n",
    "    glove = {}\n",
    "    with open(path, mode='r', encoding='utf-8') as file:\n",
    "        for line in tqdm.tqdm(file):\n",
    "            values = line.split(' ')\n",
    "            glove[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "        return glove\n",
    "    raise FileNotFoundError"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BDX8b2EJm1uZ",
    "outputId": "c477bc5e-4b5f-4880-8916-fa53fd7ef300",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%time\n",
    "glove = load_glove(path='./data/glove.840B.300d.txt')"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2196017it [02:36, 14003.51it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 5.71 s, total: 2min 30s\n",
      "Wall time: 2min 36s\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ELVYepbbm7ny"
   },
   "source": [
    "def load_embeddings(glove, text_vocab, embedding_size=300, most_common=1000):\n",
    "    most_common_words = [*map(lambda x: x[0], text_vocab.word2count.most_common(most_common))]\n",
    "    most_common_words = [*filter(lambda word: word in text_vocab.vocab, most_common_words)]\n",
    "    embedding_matrix = np.zeros((len(text_vocab), embedding_size))\n",
    "    most_common_indexes, n_words = [], 0\n",
    "    for index, word in enumerate(text_vocab.vocab):\n",
    "        if word in most_common_words:\n",
    "            most_common_indexes.append(index)\n",
    "        try:\n",
    "            embedding_matrix[index] = glove[word]\n",
    "            n_words += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix, n_words, most_common_indexes"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3CUDosl_n5HA",
    "outputId": "ce6edfc0-2a4e-41ba-dc6d-8ddf703b6501",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "embedding_matrix, n_words, most_common_indexes = load_embeddings(glove, text_vocab=TEXT)\n",
    "print(f'Words found: {n_words}/{len(TEXT)}')\n",
    "np.save('./data/GloVe_DrQA.npy', embedding_matrix)"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Words found: 25414/26885\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JZzbAOFAmfnT"
   },
   "source": [
    "# Free up the RAM\n",
    "del glove\n",
    "del embedding_matrix"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxnYdks-L31E"
   },
   "source": [
    "## Modeling\n",
    "\n",
    "***Stacked Bidirectional LSTM Layer***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DoHYvqQQCEGw"
   },
   "source": [
    "class StackedBiLSTMsLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, n_layers, dropout):\n",
    "        super(StackedBiLSTMsLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstms = nn.ModuleList([nn.LSTM(embedding_size if i == 0 else hidden_size * 2, hidden_size,\n",
    "                                            batch_first=True, num_layers=n_layers, bidirectional=True)\n",
    "                                    for i in range(n_layers)])\n",
    "    \n",
    "    def apply_lstm(self, layer, inputs, lengths):\n",
    "        \"\"\"\n",
    "        :param nn.LSTM layer\n",
    "        :param FloatTensor[batch_size, seq_len, embedding_size | hidden_size * 2] inputs\n",
    "        :param LongTensor[batch_size, seq_len] lengths\n",
    "        :return FloatTensor[batch_size, seq_len, hidden_size * 2] out_padded\n",
    "        \"\"\"\n",
    "        inputs = self.dropout(inputs)\n",
    "        packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = layer(packed)\n",
    "        out_padded, out_lengths = pad_packed_sequence(out_packed, batch_first=True) # [batch_size, seq_len, hidden_size * 2]\n",
    "        return out_padded, out_lengths\n",
    "    \n",
    "    def forward(self, input_embedded, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :param FloatTensor[batch_size, seq_len, embedding_size] input_embedded\n",
    "        :param LongTensor[batch_size, seq_len] sequence_lengths\n",
    "        :return FloatTensor[batch_size, seq_len, hidden_size * n_layers * 2]\n",
    "        \"\"\"\n",
    "        outputs, lens = [input_embedded], sequence_lengths\n",
    "        for lstm in self.lstms:\n",
    "            out, lens = self.apply_lstm(layer=lstm, inputs=outputs[-1], lengths=lens)\n",
    "            outputs.append(out)\n",
    "        return self.dropout(torch.cat(outputs[1:], dim=-1))"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oazq30eW3_T"
   },
   "source": [
    "***Aligned Question Embedding Layer***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ATOQC6-pWzq_"
   },
   "source": [
    "class AlignQuestionEmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AlignQuestionEmbeddingLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, ctx_embed, qst_embed, qst_mask):\n",
    "        \"\"\"\n",
    "        :param FloatTensor[batch_size, ctx_len, embedding_size] ctx_embed\n",
    "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
    "        :param IntTensor[batch_size, qst_len] qst_mask\n",
    "        :return FloatTensor[batch_size, ctx_len, hidden_size]\n",
    "        \"\"\"\n",
    "        ctx_embed = F.relu(self.linear(ctx_embed)) # [batch_size, ctx_len, hidden_size]\n",
    "        qst_embed = F.relu(self.linear(qst_embed)) # [batch_size, qst_len, hidden_size]\n",
    "        scores = torch.bmm(ctx_embed, qst_embed.transpose(-1, -2)) # [batch_size, ctx_len, qst_len]\n",
    "        scores = scores.masked_fill(qst_mask.unsqueeze(1) == 0, 1e-18)\n",
    "        attention_weights = F.softmax(scores, dim=-1) # [batch_size, ctx_len, qst_len]\n",
    "        return torch.bmm(attention_weights, qst_embed)"
   ],
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImX8ayztYjsK"
   },
   "source": [
    "***Question Encoding Layer***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Whil-FgVYhgy"
   },
   "source": [
    "class QuestionEncodingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, dropout, n_layers):\n",
    "        super(QuestionEncodingLayer, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.stacked_bilstms_layer = StackedBiLSTMsLayer(embedding_size=embedding_size, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(embedding_size, 1)\n",
    "    \n",
    "    def linear_self_attention(self, qst_embed, qst_mask):\n",
    "        \"\"\"\n",
    "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
    "        :param IntTensor[batch_size, qst_len] qst_mask\n",
    "        :return FloatTensor[batch_size, qst_len]\n",
    "        \"\"\"\n",
    "        scores = self.linear(qst_embed).squeeze(-1) # [batch_size, qst_len]\n",
    "        scores = scores.masked_fill(qst_mask == 0, 1e-18)\n",
    "        return F.softmax(scores, dim=-1)\n",
    "    \n",
    "    def forward(self, qst_embed, qst_lengths, qst_mask):\n",
    "        \"\"\"\n",
    "        :param FloatTensor[batch_size, qst_len, embedding_size] qst_embed\n",
    "        :param IntTensor[batch_size, qst_len] qst_lengths\n",
    "        :param IntTensor[batch_size, qst_len] qst_mask\n",
    "        :return FloatTensor[batch_size, hidden_size * n_layers * 2]\n",
    "        \"\"\"\n",
    "        attention_weights = self.linear_self_attention(qst_embed=qst_embed, qst_mask=qst_mask) # [batch_size, qst_len]\n",
    "        lstm_outputs = self.stacked_bilstms_layer(input_embedded=qst_embed, sequence_lengths=qst_lengths)\n",
    "        # lstm_outputs: [batch_size, qst_len, hidden_size * n_layers * 2]\n",
    "        return torch.bmm(attention_weights.unsqueeze(1), lstm_outputs).squeeze(1)"
   ],
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hFIZnmUaFAu"
   },
   "source": [
    "***BiLinear Attention Layer***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JZGOH3IhaEZw"
   },
   "source": [
    "class BiLinearAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, ctx_size, qst_size):\n",
    "        super(BiLinearAttentionLayer, self).__init__()\n",
    "        self.ctx_size = ctx_size\n",
    "        self.qst_size = qst_size\n",
    "        self.linear = nn.Linear(qst_size, ctx_size)\n",
    "    \n",
    "    def forward(self, ctx_encoded, qst_encoded, ctx_mask):\n",
    "        \"\"\"\n",
    "        :param FloatTensor[batch_size, ctx_len, ctx_size] ctx_encoded\n",
    "        :param FloatTensor[batch_size, qst_size] qst_encoded\n",
    "        :param IntTensor[batch_size, ctx_len] ctx_mask\n",
    "        :return FloatTensor[batch_size, ctx_len, hidden_size]\n",
    "        \"\"\"\n",
    "        qst_encoded = self.linear(qst_encoded) # [batch_size, ctx_size]\n",
    "        scores = torch.bmm(ctx_encoded, qst_encoded.unsqueeze(-1)) # [batch_size, ctx_len, 1]\n",
    "        scores = scores.squeeze(-1).masked_fill(ctx_mask == 0, 1e-18) # [batch_size, ctx_len]\n",
    "        return scores"
   ],
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzrmk6D7agHH"
   },
   "source": [
    "***Document reader Question Answering Model***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qlZlTFXOae_o"
   },
   "source": [
    "class DrQA(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, n_extra_features, hidden_size, n_layers, dropout, pad_index):\n",
    "        super(DrQA, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_extra_features = n_extra_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pad_index = pad_index\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_index)\n",
    "        self.align_question_embedding_layer = AlignQuestionEmbeddingLayer(hidden_size=embedding_size)\n",
    "        self.ctx_stacked_bi_lstm_layer = StackedBiLSTMsLayer(embedding_size=embedding_size * 2 + n_extra_features,\n",
    "                                                             hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        self.qst_encoding_layer = QuestionEncodingLayer(embedding_size=embedding_size, hidden_size=hidden_size, dropout=dropout, n_layers=n_layers)\n",
    "        self.bilinear_attention_layer_start = BiLinearAttentionLayer(ctx_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
    "        self.bilinear_attention_layer_end = BiLinearAttentionLayer(ctx_size=hidden_size * n_layers * 2, qst_size=hidden_size * n_layers * 2)\n",
    "    \n",
    "    def load_glove_embeddings(self, path, most_common_indexes, tune=True):\n",
    "        def tune_embeddings(grad, words=most_common_indexes):\n",
    "            grad[most_common_indexes] = 0\n",
    "            return grad\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(np.load(path)))\n",
    "        if tune:\n",
    "            self.embedding.weight.register_hook(tune_embeddings) # Only fine-tune the 1000 most frequent question words\\n\n",
    "    \n",
    "    def make_ctx_mask(self, ctx_sequences):\n",
    "        \"\"\"\n",
    "        :param LongTensor[batch_size, ctx_len] ctx_sequences\n",
    "        :return IntTensor[batch_size, ctx_len]\n",
    "        \"\"\"\n",
    "        return ctx_sequences != self.pad_index\n",
    "    \n",
    "    def make_qst_mask(self, qst_sequences):\n",
    "        \"\"\"\n",
    "        :param LongTensor[batch_size, qst_len] qst_sequences\n",
    "        :return IntTensor[batch_size, qst_len]\n",
    "        \"\"\"\n",
    "        return qst_sequences != self.pad_index\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(starts, ends):\n",
    "        \"\"\"\n",
    "        :param IntTensor[batch_size, ctx_len] starts\n",
    "        :param IntTensor[batch_size, ctx_len] ends\n",
    "        :return list(int) start_indexes\n",
    "        :return list(int) end_indexes\n",
    "        :return list(float) pred_probas\n",
    "        \"\"\"\n",
    "        start_indexes, end_indexes, pred_probas = [], [], []\n",
    "        for i in range(starts.size(0)):\n",
    "            probas = torch.ger(starts[i], ends[i]) # [ctx_len, ctx_len]\n",
    "            proba, index = torch.topk(probas.view(-1), k=1)\n",
    "            start_indexes.append(index.tolist()[0] // probas.size(0))\n",
    "            end_indexes.append(index.tolist()[0] % probas.size(1))\n",
    "            pred_probas.append(proba.tolist()[0])\n",
    "        return start_indexes, end_indexes, pred_probas\n",
    "    \n",
    "    def forward(self, ctx_sequences, ctx_lengths, qst_sequences, qst_lengths, em_sequences, pos_sequences, ner_sequences, ntf_sequences):\n",
    "        \"\"\"\n",
    "        :param LongTensor[batch_size, ctx_len] ctx_sequences\n",
    "        :param Tensor[batch_size,] ctx_lengths\n",
    "        :param LongTensor[batch_size, qst_len] qst_sequences\n",
    "        :param Tensor[batch_size,] qst_lengths\n",
    "        :param LongTensor[batch_size, ctx_len] em_sequences\n",
    "        :param LongTensor[batch_size, ctx_len] pos_sequences\n",
    "        :param LongTensor[batch_size, ctx_len] ner_sequences\n",
    "        :param LongTensor[batch_size, ctx_len] ntf_sequences\n",
    "        :return Tensor[batch_size, ctx_len] starts\n",
    "        :return Tensor[batch_size, ctx_len] ends\n",
    "        \"\"\"\n",
    "        ctx_mask = self.make_ctx_mask(ctx_sequences) # [batch_size, ctx_len]\n",
    "        qst_mask = self.make_qst_mask(qst_sequences) # [batch_size, qst_len]\n",
    "        ctx_embedded = self.dropout(self.embedding(ctx_sequences)) # [batch_size, ctx_len, embedding_size]\n",
    "        qst_embedded = self.dropout(self.embedding(qst_sequences)) # [batch_size, ctx_len, embedding_size]\n",
    "        ctx_aligned = self.align_question_embedding_layer(ctx_embed=ctx_embedded, qst_embed=qst_embedded,\n",
    "                                                          qst_mask=qst_mask) # [batch_size, ctx_len, embedding_size]\n",
    "        ctx_inputs = torch.cat([ctx_embedded, em_sequences.unsqueeze(-1), pos_sequences.unsqueeze(-1), ner_sequences.unsqueeze(-1),\n",
    "                                ntf_sequences.unsqueeze(-1), ctx_aligned], dim=-1) # [batch_size, ctx_len, embedding_size * 2 + 4]\n",
    "        ctx_encoded = self.ctx_stacked_bi_lstm_layer(input_embedded=ctx_inputs, sequence_lengths=ctx_lengths)\n",
    "        # ctx_encoded: [batch_size, ctx_len, hidden_size * n_layers * 2]\n",
    "        qst_encoded = self.qst_encoding_layer(qst_embed=qst_embedded, qst_lengths=qst_lengths, qst_mask=qst_mask)\n",
    "        # qst_encoded: [batch_size, hidden_size * n_layers * 2]\n",
    "        starts = self.bilinear_attention_layer_start(ctx_encoded=ctx_encoded, qst_encoded=qst_encoded, ctx_mask=ctx_mask)\n",
    "        ends = self.bilinear_attention_layer_end(ctx_encoded=ctx_encoded, qst_encoded=qst_encoded, ctx_mask=ctx_mask)\n",
    "        return starts, ends"
   ],
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x575iXlz3w4m"
   },
   "source": [
    "***Training routines***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "31k5BZf43uV4"
   },
   "source": [
    "class AverageMeter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.value = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "        self.average = 0.\n",
    "        \n",
    "    def reset(self):\n",
    "        self.value = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "        self.average = 0.\n",
    "        \n",
    "    def update(self, value, n=1):\n",
    "        self.value = value\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "        self.average = self.sum / self.count"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qSmh2GmU4KXH"
   },
   "source": [
    "def normalize(answer: str):\n",
    "    \"\"\"Performs a series of cleaning steps on the ground truth and predicted answer.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(answer))))"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hi2JHYTX6F6A"
   },
   "source": [
    "def get_scores(prediction: str, ground_truth: str):\n",
    "    prediction, ground_truth = normalize(prediction), normalize(ground_truth)\n",
    "    em_score = prediction == ground_truth\n",
    "\n",
    "    prediction_tokens, ground_truth_tokens = prediction.split(), ground_truth.split()\n",
    "    common = collections.Counter(prediction_tokens) & collections.Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        precision = 1.0 * num_same / len(prediction_tokens)\n",
    "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return em_score, f1_score"
   ],
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tv9kxaI34_Oo"
   },
   "source": [
    "def max_metrics_over_ground_truths(prediction: str, ground_truths: list):\n",
    "    scores = [get_scores(prediction, ground_truth) for ground_truth in ground_truths]\n",
    "    em_score = max(scores, key=lambda score: score[0])[0]\n",
    "    f1_score = max(scores, key=lambda score: score[1])[1]\n",
    "    return em_score, f1_score"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KmxMG01B5JEx"
   },
   "source": [
    "def metrics(predictions: dict, qas=valid_qas):\n",
    "    ground_truths = collections.defaultdict(lambda: [])\n",
    "    for qa in qas:\n",
    "        if qa['id'] in predictions:\n",
    "            ground_truths[qa['id']].append(qa['answer'].text)\n",
    "\n",
    "    em_scores, f1_scores, total = [], [], 0\n",
    "    for id in predictions:\n",
    "        em_score, f1_score = max_metrics_over_ground_truths(predictions[id], ground_truths[id])\n",
    "        em_scores.append(em_score); f1_scores.append(f1_score)\n",
    "        total += 1\n",
    "\n",
    "    em_score = 100.0 * sum(em_scores) / total\n",
    "    f1_score = 100.0 * sum(f1_scores) / total\n",
    "    return em_score, f1_score"
   ],
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GHUdYB6f-NRJ"
   },
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, id_field, text_field):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.id_field = id_field\n",
    "        self.text_field = text_field\n",
    "        \n",
    "    def train_step(self, loader, epoch, grad_clip):\n",
    "        loss_tracker = AverageMeter()\n",
    "        self.model.train()\n",
    "        progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, batch in progress_bar:\n",
    "            self.optimizer.zero_grad()\n",
    "            starts, ends = self.model(*batch.ctx, *batch.qst, batch.em, batch.pos, batch.ner, batch.ntf) # [batch_size, ctx_len]\n",
    "            loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "            self.optimizer.step()\n",
    "            loss_tracker.update(loss.item())\n",
    "            progress_bar.set_description(f'Epoch: {epoch+1:02d} -     loss: {loss_tracker.average:.3f}')\n",
    "        return loss_tracker.average\n",
    "    \n",
    "    def validate(self, loader, epoch):\n",
    "        loss_tracker, predictions = AverageMeter(), {}\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "            for i, batch in progress_bar:\n",
    "                starts, ends = self.model(*batch.ctx, *batch.qst, batch.em, batch.pos, batch.ner, batch.ntf) # [batch_size, ctx_len]\n",
    "                loss = self.criterion(starts, batch.trg[:, 0]) + self.criterion(ends, batch.trg[:, 1])\n",
    "                start_indexes, end_indexes, _ = DrQA.decode(starts=F.softmax(starts, dim=-1), ends=F.softmax(ends, dim=-1))\n",
    "                for i in range(starts.size(0)):\n",
    "                    id = self.id_field.itos(batch.id[i].item())\n",
    "                    prediction = batch.ctx[0][i][start_indexes[i]:end_indexes[i]+1]\n",
    "                    predictions[id] = ' '.join([self.text_field.itos(indice.item()) for indice in prediction])\n",
    "                loss_tracker.update(loss.item())\n",
    "                progress_bar.set_description(f'Epoch: {epoch+1:02d} - val_loss: {loss_tracker.average:.3f}')\n",
    "        return loss_tracker.average, predictions\n",
    "    \n",
    "    def train(self, train_loader, valid_loader, n_epochs, grad_clip):\n",
    "        history, best_loss = {'loss': [], 'val_loss': [], 'em': [], 'f1': []}, float('inf')\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train_step(train_loader, epoch, grad_clip)\n",
    "            val_loss, predictions = self.validate(valid_loader, epoch)\n",
    "            em_score, f1_score = metrics(predictions)\n",
    "            history['loss'].append(loss); history['val_loss'].append(val_loss)\n",
    "            history['em'].append(em_score); history['f1'].append(f1_score)\n",
    "            if best_loss > val_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), './checkpoints/DrQA.pth')\n",
    "            time.sleep(1)\n",
    "            print(f'\\nEM={em_score:.3f}% - F1={f1_score:.3f}%')\n",
    "        return history"
   ],
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFzM-vWUAov9"
   },
   "source": [
    "***Train the model***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-YCmJQLhAlqo"
   },
   "source": [
    "N_LAYERS = 3\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.3\n",
    "N_EPOCHS = 5\n",
    "GRAD_CLIP = 1.0"
   ],
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ersGVmFxBwAV",
    "outputId": "f5c11bc4-a4bb-49cb-8fc1-c472323ec253",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "drqa = DrQA(vocab_size=len(TEXT),\n",
    "            embedding_size=EMBED_SIZE,\n",
    "            n_extra_features=4,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            n_layers=N_LAYERS,\n",
    "            dropout=DROPOUT,\n",
    "            pad_index=TEXT.stoi(PAD_TOKEN))\n",
    "drqa.load_glove_embeddings('./data/GloVe_DrQA.npy', most_common_indexes, tune=True)\n",
    "drqa.to(DEVICE)\n",
    "optimizer = optim.Adamax(params=drqa.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TEXT.stoi(PAD_TOKEN))\n",
    "print(f'Number of parameters of the model: {sum(p.numel() for p in drqa.parameters() if p.requires_grad):,}')\n",
    "print(drqa)\n",
    "trainer = Trainer(model=drqa, optimizer=optimizer, criterion=criterion, id_field=ID, text_field=TEXT)"
   ],
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 16,853,445\n",
      "DrQA(\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (embedding): Embedding(26885, 300, padding_idx=1318)\n",
      "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
      "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (ctx_stacked_bi_lstm_layer): StackedBiLSTMsLayer(\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (lstms): ModuleList(\n",
      "      (0): LSTM(604, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "      (1): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "      (2): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (qst_encoding_layer): QuestionEncodingLayer(\n",
      "    (stacked_bilstms_layer): StackedBiLSTMsLayer(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (lstms): ModuleList(\n",
      "        (0): LSTM(300, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "        (1): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "        (2): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "giP1Ov5_DTGr",
    "outputId": "aa776c3f-80f1-4c9f-9abb-37e38eb2841c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!mkdir -p ./checkpoints\n",
    "history = trainer.train(train_loader=train_dataloader, valid_loader=valid_dataloader, n_epochs=N_EPOCHS, grad_clip=GRAD_CLIP)"
   ],
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 01 -     loss: 5.694: 100%|██████████| 2707/2707 [11:41<00:00,  3.86it/s]\n",
      "Epoch: 01 - val_loss: 4.474: 100%|██████████| 1072/1072 [01:54<00:00,  9.33it/s]\n",
      "  0%|          | 0/2707 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "EM=28.195% - F1=40.971%\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: 02 -     loss: 4.265: 100%|██████████| 2707/2707 [11:48<00:00,  3.82it/s]\n",
      "Epoch: 02 - val_loss: 4.026: 100%|██████████| 1072/1072 [01:51<00:00,  9.60it/s]\n",
      "  0%|          | 0/2707 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "EM=33.581% - F1=47.180%\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: 03 -     loss: 3.755: 100%|██████████| 2707/2707 [11:44<00:00,  3.84it/s]\n",
      "Epoch: 03 - val_loss: 3.688: 100%|██████████| 1072/1072 [01:51<00:00,  9.61it/s]\n",
      "  0%|          | 0/2707 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "EM=36.397% - F1=50.769%\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: 04 -     loss: 3.452: 100%|██████████| 2707/2707 [11:42<00:00,  3.85it/s]\n",
      "Epoch: 04 - val_loss: 3.619: 100%|██████████| 1072/1072 [01:52<00:00,  9.53it/s]\n",
      "  0%|          | 0/2707 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "EM=37.996% - F1=52.712%\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: 05 -     loss: 3.234: 100%|██████████| 2707/2707 [11:42<00:00,  3.86it/s]\n",
      "Epoch: 05 - val_loss: 3.550: 100%|██████████| 1072/1072 [01:50<00:00,  9.68it/s]\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "EM=38.615% - F1=53.212%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8C-bSJpY-qZM",
    "outputId": "db26fdee-279c-4fe8-b5d6-7f26a8587271",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    }
   },
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['em'], label='valid')\n",
    "axes[1].set_title('Exact match history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Exact match (%)')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['f1'], label='valid')\n",
    "axes[2].set_title('F1 history')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 (%)')\n",
    "axes[2].grid(True)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "tags": [],
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_99dtAVn5CL"
   },
   "source": [
    "***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gk9dSwSR_OM9",
    "outputId": "dfc3f33a-a716-44fd-ae2d-3ce5fdaa745f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "drqa.load_state_dict(torch.load('./checkpoints/DrQA.pth'))\n",
    "drqa.to(DEVICE)"
   ],
   "execution_count": 59,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DrQA(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (embedding): Embedding(26885, 300, padding_idx=1318)\n",
       "  (align_question_embedding_layer): AlignQuestionEmbeddingLayer(\n",
       "    (linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (ctx_stacked_bi_lstm_layer): StackedBiLSTMsLayer(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstms): ModuleList(\n",
       "      (0): LSTM(604, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "      (2): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       "  (qst_encoding_layer): QuestionEncodingLayer(\n",
       "    (stacked_bilstms_layer): StackedBiLSTMsLayer(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (lstms): ModuleList(\n",
       "        (0): LSTM(300, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "        (1): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "        (2): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       "  (bilinear_attention_layer_start): BiLinearAttentionLayer(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (bilinear_attention_layer_end): BiLinearAttentionLayer(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 59
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p1CVgakLDtsU"
   },
   "source": [
    "def inference(model: nn.Module, context: spacy.tokens.doc.Doc, question: spacy.tokens.doc.Doc,\n",
    "              text_vocab: Vocab, pos_vocab: Vocab, ner_vocab: Vocab, device: torch.device):\n",
    "    # Build extra features\n",
    "    question = [token.text.lower() for token in question]\n",
    "    counts = collections.Counter(map(lambda token: token.text.lower(), context))\n",
    "    freqs = {index: counts[token.text.lower()] for index, token in enumerate(context)}\n",
    "    freqs_norm = sum(freqs.values())\n",
    "    em, pos, ner, ntf = zip(\n",
    "        *map(lambda index: [\n",
    "            context[index].text.lower() in question, context[index].tag_,\n",
    "            context[index].ent_type_ or 'None',\n",
    "            freqs[index] / freqs_norm\n",
    "        ], range(len(context)))\n",
    "    )\n",
    "\n",
    "    # Build tensors\n",
    "    ctx = torch.LongTensor([*map(lambda word: text_vocab.stoi(word), context)]).unsqueeze(0).to(device)\n",
    "    qst = torch.LongTensor([*map(lambda word: text_vocab.stoi(word), question)]).unsqueeze(0).to(device)\n",
    "    len_ctx = torch.LongTensor([len(context)]).to(device)\n",
    "    len_qst = torch.LongTensor([len(question)]).to(device)\n",
    "    em = torch.LongTensor(em).unsqueeze(0).to(device)\n",
    "    pos = torch.LongTensor([*map(lambda x: pos_vocab.stoi(x), pos)]).unsqueeze(0).to(device)\n",
    "    ner = torch.LongTensor([*map(lambda x: ner_vocab.stoi(x), ner)]).unsqueeze(0).to(device)\n",
    "    ntf = torch.LongTensor(ntf).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Feed the model\n",
    "        start, end = model(ctx_sequences=ctx, ctx_lengths=len_ctx, qst_sequences=qst, qst_lengths=len_qst,\n",
    "                           em_sequences=em, pos_sequences=pos, ner_sequences=ner, ntf_sequences=ntf)\n",
    "    \n",
    "        # Decode the result indexes\n",
    "        start_index, end_index, proba = model.__class__.decode(starts=F.softmax(start, dim=-1), ends=F.softmax(end, dim=-1))\n",
    "\n",
    "        # Extract the answer\n",
    "        answer = context[start_index[0]:end_index[0] + 1]\n",
    "\n",
    "    return answer, proba[0]"
   ],
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dI-ME2Q1-WzB",
    "outputId": "7ddd80ef-e885-476d-a8e7-32e1a031417b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "for index in np.random.choice(len(valid_qas), size=25, replace=False):\n",
    "    id = valid_qas[index]['id']\n",
    "    context = valid_qas[index]['context']\n",
    "    question = valid_qas[index]['question']\n",
    "\n",
    "    answers = []\n",
    "    for qa in valid_qas:\n",
    "        if id == qa['id']:\n",
    "            answers.append(qa['answer'])\n",
    "\n",
    "    prediction, proba = inference(model=drqa, context=context, question=question,\n",
    "                                  text_vocab=TEXT, pos_vocab=POS, ner_vocab=NER, device=DEVICE)\n",
    "    \n",
    "    html = f'<p><span><b>Context:</b> {context.text}</span><br />'\n",
    "    html += f'<span><b>Question:</b> {question.text}</span><br />'\n",
    "    html += f'<span style=\"color:blue\"><b>Possible answers:</b><br /><ul>'\n",
    "    for answer in answers:\n",
    "        html += f'<li style=\"color:blue\">{answer.text}</li>'\n",
    "    html += '</ul></span><br />'\n",
    "    html += f'<span style=\"color:green\"><b>Prediction:</b> {prediction}</span><br />'\n",
    "    html += f'<span style=\"color:green\"><b>Probability:</b> {proba * 100:.3f}%</span><br />'\n",
    "    display(HTML(html))\n",
    "    print('='*100)"
   ],
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> About the time of the first landing in 1969, it was decided to use an existing Saturn V to launch the Skylab orbital laboratory pre-built on the ground, replacing the original plan to construct it in orbit from several Saturn IB launches; this eliminated Apollo 20. NASA's yearly budget also began to shrink in light of the successful landing, and NASA also had to make funds available for the development of the upcoming Space Shuttle. By 1971, the decision was made to also cancel missions 18 and 19. The two unused Saturn Vs became museum exhibits at the John F. Kennedy Space Center on Merritt Island, Florida, George C. Marshall Space Center in Huntsville, Alabama, Michoud Assembly Facility in New Orleans, Louisiana, and Lyndon B. Johnson Space Center in Houston, Texas.</span><br /><span><b>Question:</b> After Apollo missions 18 and 19 were cancelled, what happened to the Saturn Vs that were never used?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">museum exhibits</li><li style=\"color:blue\">museum exhibits</li><li style=\"color:blue\">museum exhibits</li><li style=\"color:blue\">museum exhibits</li><li style=\"color:blue\">museum exhibits</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> museum exhibits</span><br /><span style=\"color:green\"><b>Probability:</b> 2.860%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Plastoglobuli (singular plastoglobulus, sometimes spelled plastoglobule(s)), are spherical bubbles of lipids and proteins about 45–60 nanometers across. They are surrounded by a lipid monolayer. Plastoglobuli are found in all chloroplasts, but become more common when the chloroplast is under oxidative stress, or when it ages and transitions into a gerontoplast. Plastoglobuli also exhibit a greater size variation under these conditions. They are also common in etioplasts, but decrease in number as the etioplasts mature into chloroplasts.</span><br /><span><b>Question:</b> What surrounds Plastoglobuli?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">a lipid monolayer</li><li style=\"color:blue\">lipid monolayer</li><li style=\"color:blue\">a lipid monolayer</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> spherical bubbles of lipids and proteins</span><br /><span style=\"color:green\"><b>Probability:</b> 15.903%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> A second period of international expansion is linked to that of the ESPN network in the 1990s, and policies enacted in the 2000s by Disney Media Networks (which included the expansion of several of the company's U.S.-based cable networks including Disney Channel and its spinoffs Toon Disney, Playhouse Disney and Jetix; although Disney also sold its 33% stake in European sports channel Eurosport for $155 million in June 2000). In contrast to Disney's other channels, ABC is broadcast in the United States, although the network's programming is syndicated in many countries. The policy regarding wholly owned international networks was revived in 2004 when on September 27 of that year, ABC announced the launch of ABC1, a free-to-air channel in the United Kingdom owned by the ABC Group. However, on September 8, 2007, Disney announced that it would discontinue ABC1 citing to the channel's inability to attain sustainable viewership. With ABC1's shutdown that October, the company's attempt to develop ABC International were discontinued.</span><br /><span><b>Question:</b> What developmental network was discontinued after the shutdown of ABC1?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">ABC International</li><li style=\"color:blue\">ABC International</li><li style=\"color:blue\">ABC International</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> October, the company's attempt to develop ABC International</span><br /><span style=\"color:green\"><b>Probability:</b> 12.383%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Oxygen gas (O\n",
       "2) can be toxic at elevated partial pressures, leading to convulsions and other health problems.[j] Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O\n",
       "2 partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%–50% O\n",
       "2 by volume (about 30 kPa at standard pressure). (although this figure also is subject to wide variation, depending on type of mask).</span><br /><span><b>Question:</b> When can oxygen gas produce a toxic condition?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">at elevated partial pressures</li><li style=\"color:blue\">elevated partial pressures</li><li style=\"color:blue\">at elevated partial pressures</li><li style=\"color:blue\">at elevated partial pressures</li><li style=\"color:blue\">elevated partial pressures</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> at elevated partial pressures</span><br /><span style=\"color:green\"><b>Probability:</b> 21.671%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> On 19 October 1512, he was awarded his Doctor of Theology and, on 21 October 1512, was received into the senate of the theological faculty of the University of Wittenberg, having been called to the position of Doctor in Bible. He spent the rest of his career in this position at the University of Wittenberg.</span><br /><span><b>Question:</b> Where did Luther spend his career?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">University of Wittenberg</li><li style=\"color:blue\">University of Wittenberg</li><li style=\"color:blue\">University of Wittenberg.</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> University of Wittenberg</span><br /><span style=\"color:green\"><b>Probability:</b> 25.102%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.</span><br /><span><b>Question:</b> How many miners died in the typhoid outbreak of 1854?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">over 1,000</li><li style=\"color:blue\">1,000</li><li style=\"color:blue\">1,000</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> over 1,000</span><br /><span style=\"color:green\"><b>Probability:</b> 46.071%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Tesla obtained around 300 patents worldwide for his inventions. Some of Tesla's patents are not accounted for, and various sources have discovered some that have lain hidden in patent archives. There are a minimum of 278 patents issued to Tesla in 26 countries that have been accounted for. Many of Tesla's patents were in the United States, Britain, and Canada, but many other patents were approved in countries around the globe.:62 Many inventions developed by Tesla were not put into patent protection.</span><br /><span><b>Question:</b> Other than the US and Britain what was the other main country that Tesla had patents granted?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">Canada</li><li style=\"color:blue\">Canada</li><li style=\"color:blue\">Canada</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Canada</span><br /><span style=\"color:green\"><b>Probability:</b> 3.089%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Most of the Huguenot congregations (or individuals) in North America eventually affiliated with other Protestant denominations with more numerous members. The Huguenots adapted quickly and often married outside their immediate French communities, which led to their assimilation. Their descendants in many families continued to use French first names and surnames for their children well into the nineteenth century. Assimilated, the French made numerous contributions to United States economic life, especially as merchants and artisans in the late Colonial and early Federal periods. For example, E.I. du Pont, a former student of Lavoisier, established the Eleutherian gunpowder mills.</span><br /><span><b>Question:</b> How did Huguenots evolve their religious beliefs in the New World?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">affiliated with other Protestant denominations</li><li style=\"color:blue\">affiliated with other Protestant denominations</li><li style=\"color:blue\">affiliated with other Protestant denominations with more numerous members</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> led to their assimilation</span><br /><span style=\"color:green\"><b>Probability:</b> 16.719%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Sometimes the prosecution proposes a plea bargain to civil disobedients, as in the case of the Camden 28, in which the defendants were offered an opportunity to plead guilty to one misdemeanor count and receive no jail time. In some mass arrest situations, the activists decide to use solidarity tactics to secure the same plea bargain for everyone. But some activists have opted to enter a blind plea, pleading guilty without any plea agreement in place. Mohandas Gandhi pleaded guilty and told the court, \"I am here to . . . submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen.\"</span><br /><span><b>Question:</b> When many people are arrested, what is a common tactic negotiating?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">solidarity tactics</li><li style=\"color:blue\">solidarity tactics</li><li style=\"color:blue\">solidarity</li><li style=\"color:blue\">solidarity</li><li style=\"color:blue\">solidarity</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> In</span><br /><span style=\"color:green\"><b>Probability:</b> 24.839%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> In 1755, six colonial governors in North America met with General Edward Braddock, the newly arrived British Army commander, and planned a four-way attack on the French. None succeeded and the main effort by Braddock was a disaster; he was defeated in the Battle of the Monongahela on July 9, 1755 and died a few days later. British operations in 1755, 1756 and 1757 in the frontier areas of Pennsylvania and New York all failed, due to a combination of poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies. In 1755, the British captured Fort Beauséjour on the border separating Nova Scotia from Acadia; soon afterward they ordered the expulsion of the Acadians. Orders for the deportation were given by William Shirley, Commander-in-Chief, North America, without direction from Great Britain. The Acadians, both those captured in arms and those who had sworn the loyalty oath to His Britannic Majesty, were expelled. Native Americans were likewise driven off their land to make way for settlers from New England.</span><br /><span><b>Question:</b> What order did British make of French?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">expulsion of the Acadians</li><li style=\"color:blue\">expulsion of the Acadians</li><li style=\"color:blue\">deportation</li><li style=\"color:blue\">expulsion</li><li style=\"color:blue\">deportation</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies</span><br /><span style=\"color:green\"><b>Probability:</b> 1.648%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> The United Methodist Church, along with other Methodist churches, condemns capital punishment, saying that it cannot accept retribution or social vengeance as a reason for taking human life. The Church also holds that the death penalty falls unfairly and unequally upon marginalized persons including the poor, the uneducated, ethnic and religious minorities, and persons with mental and emotional illnesses. The United Methodist Church also believes that Jesus explicitly repudiated the lex talionis in Matthew 5:38-39 and abolished the death penalty in John 8:7. The General Conference of the United Methodist Church calls for its bishops to uphold opposition to capital punishment and for governments to enact an immediate moratorium on carrying out the death penalty sentence.</span><br /><span><b>Question:</b> The UMC condemns what type of punishment?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">capital punishment</li><li style=\"color:blue\">capital punishment</li><li style=\"color:blue\">capital punishment</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> capital</span><br /><span style=\"color:green\"><b>Probability:</b> 79.706%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Chloroplasts' main role is to conduct photosynthesis, where the photosynthetic pigment chlorophyll captures the energy from sunlight and converts it and stores it in the energy-storage molecules ATP and NADPH while freeing oxygen from water. They then use the ATP and NADPH to make organic molecules from carbon dioxide in a process known as the Calvin cycle. Chloroplasts carry out a number of other functions, including fatty acid synthesis, much amino acid synthesis, and the immune response in plants. The number of chloroplasts per cell varies from 1 in algae up to 100 in plants like Arabidopsis and wheat.</span><br /><span><b>Question:</b> What is the process of turning CO2 into organic molecules called?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">the Calvin cycle</li><li style=\"color:blue\">Calvin cycle</li><li style=\"color:blue\">Calvin cycle</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Calvin cycle</span><br /><span style=\"color:green\"><b>Probability:</b> 57.625%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> On May 21, 2013, NFL owners at their spring meetings in Boston voted and awarded the game to Levi's Stadium. The $1.2 billion stadium opened in 2014. It is the first Super Bowl held in the San Francisco Bay Area since Super Bowl XIX in 1985, and the first in California since Super Bowl XXXVII took place in San Diego in 2003.</span><br /><span><b>Question:</b> When was the last Super Bowl in California?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">2003</li><li style=\"color:blue\">in 2003</li><li style=\"color:blue\">2003</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> 2003</span><br /><span style=\"color:green\"><b>Probability:</b> 16.819%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Unlike confirmation and profession of faith, Baptism is a sacrament in the UMC. The Book of Discipline of the United Methodist Church directs the local church to offer membership preparation or confirmation classes to all people, including adults. The term confirmation is generally reserved for youth, while some variation on membership class is generally used for adults wishing to join the church. The Book of Discipline normally allows any youth at least completing sixth grade to participate, although the pastor has discretionary authority to allow a younger person to participate. In confirmation and membership preparation classes, students learn about Church and the Methodist-Christian theological tradition in order to profess their ultimate faith in Christ.</span><br /><span><b>Question:</b> What do students learn about in confirmation and membership preparation classes?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">Church and the Methodist-Christian theological tradition</li><li style=\"color:blue\">learn about Church and the Methodist-Christian theological tradition in order to profess their ultimate faith in Christ.</li><li style=\"color:blue\">Church and the Methodist-Christian theological tradition</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Church</span><br /><span style=\"color:green\"><b>Probability:</b> 75.325%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> The problems with North American were severe enough in late 1965 to cause Manned Space Flight Administrator George Mueller to appoint program director Samuel Phillips to head a \"tiger team\" to investigate North American's problems and identify corrections. Phillips documented his findings in a December 19 letter to NAA president Lee Atwood, with a strongly worded letter by Mueller, and also gave a presentation of the results to Mueller and Deputy Administrator Robert Seamans. Meanwhile, Grumman was also encountering problems with the Lunar Module, eliminating hopes it would be ready for manned flight in 1967, not long after the first manned CSM flights.</span><br /><span><b>Question:</b> Who appointed Samuel Phillips to man the tiger team to find answers?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">George Mueller</li><li style=\"color:blue\">Seamans</li><li style=\"color:blue\">George Mueller</li><li style=\"color:blue\">George Mueller</li><li style=\"color:blue\">Mueller</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Mueller</span><br /><span style=\"color:green\"><b>Probability:</b> 8.340%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> A revised version of the ABC logo was introduced for promotions for the 2013–14 season during the network's upfront presentation on May 14, 2013, and officially introduced on-air on June 17 (although some affiliates implemented the new design prior to then), as part of an overhaul of ABC's identity by design agency LoyalKaspar. The updated logo carries a simpler gloss design than the previous version, and contains lettering more closely resembling Paul Rand's original version of the circle logo. The logo is displayed on-air, online and in print advertising in four variants shading the respective color used with the circle design's native black coloring: a gold version is primarily used on entertainment-oriented outlets (such as ABC.com, WATCH ABC, and by ABC Studios) and the on-screen bug; steel blue and dark grey versions are used primarily by ABC News; a red version is used for ESPN on ABC, while all four variants are used selectively in advertising and by affiliates. A new custom typeface, \"ABC Modern\" (which was inspired by the logotype), was also created for use in advertising and other promotional materials.</span><br /><span><b>Question:</b> What new typeface was created for ABC for use in advertising?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">ABC Modern</li><li style=\"color:blue\">ABC Modern</li><li style=\"color:blue\">ABC Modern</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> ABC Modern</span><br /><span style=\"color:green\"><b>Probability:</b> 34.859%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> One of the most famous people born in Warsaw was Maria Skłodowska-Curie, who achieved international recognition for her research on radioactivity and was the first female recipient of the Nobel Prize. Famous musicians include Władysław Szpilman and Frédéric Chopin. Though Chopin was born in the village of Żelazowa Wola, about 60 km (37 mi) from Warsaw, he moved to the city with his family when he was seven months old. Casimir Pulaski, a Polish general and hero of the American Revolutionary War, was born here in 1745.</span><br /><span><b>Question:</b> Who was Frédéric Chopin?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">Famous musicians</li><li style=\"color:blue\">musicians</li><li style=\"color:blue\">Famous musicians</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Władysław Szpilman</span><br /><span style=\"color:green\"><b>Probability:</b> 14.742%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Following the success of the 2005 series produced by Russell T Davies, the BBC commissioned Davies to produce a 13-part spin-off series titled Torchwood (an anagram of \"Doctor Who\"), set in modern-day Cardiff and investigating alien activities and crime. The series debuted on BBC Three on 22 October 2006. John Barrowman reprised his role of Jack Harkness from the 2005 series of Doctor Who. Two other actresses who appeared in Doctor Who also star in the series; Eve Myles as Gwen Cooper, who also played the similarly named servant girl Gwyneth in the 2005 Doctor Who episode \"The Unquiet Dead\", and Naoko Mori who reprised her role as Toshiko Sato first seen in \"Aliens of London\". A second series of Torchwood aired in 2008; for three episodes, the cast was joined by Freema Agyeman reprising her Doctor Who role of Martha Jones. A third series was broadcast from 6 to 10 July 2009, and consisted of a single five-part story called Children of Earth which was set largely in London. A fourth series, Torchwood: Miracle Day jointly produced by BBC Wales, BBC Worldwide and the American entertainment company Starz debuted in 2011. The series was predominantly set in the United States, though Wales remained part of the show's setting.</span><br /><span><b>Question:</b> When did the second series of Torchwood play?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">2008</li><li style=\"color:blue\">2008</li><li style=\"color:blue\">2008</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> 2008</span><br /><span style=\"color:green\"><b>Probability:</b> 82.038%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> One of the most dramatic parts of the museum is the Cast Courts in the sculpture wing, comprising two large, skylighted rooms two storeys high housing hundreds of plaster casts of sculptures, friezes and tombs. One of these is dominated by a full-scale replica of Trajan's Column, cut in half in order to fit under the ceiling. The other includes reproductions of various works of Italian Renaissance sculpture and architecture, including a full-size replica of Michelangelo's David. Replicas of two earlier Davids by Donatello's David and Verrocchio's David, are also included, although for conservation reasons the Verrocchio replica is displayed in a glass case.</span><br /><span><b>Question:</b> How is the plaster replica of Verrocchio's David displayed in the Cast Courts?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">in a glass case</li><li style=\"color:blue\">glass case</li><li style=\"color:blue\">in a glass case</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> a glass case</span><br /><span style=\"color:green\"><b>Probability:</b> 21.281%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Luther was the most widely read author of his generation, and within Germany he acquired the status of a prophet. According to the prevailing view among historians, his anti-Jewish rhetoric contributed significantly to the development of antisemitism in Germany, and in the 1930s and 1940s provided an \"ideal underpinning\" for the Nazis' attacks on Jews. Reinhold Lewin writes that anybody who \"wrote against the Jews for whatever reason believed he had the right to justify himself by triumphantly referring to Luther.\" According to Michael, just about every anti-Jewish book printed in the Third Reich contained references to and quotations from Luther. Heinrich Himmler wrote admiringly of his writings and sermons on the Jews in 1940. The city of Nuremberg presented a first edition of On the Jews and their Lies to Julius Streicher, editor of the Nazi newspaper Der Stürmer, on his birthday in 1937; the newspaper described it as the most radically anti-Semitic tract ever published. It was publicly exhibited in a glass case at the Nuremberg rallies and quoted in a 54-page explanation of the Aryan Law by Dr. E.H. Schulz and Dr. R. Frercks.</span><br /><span><b>Question:</b> Whose writings were widely quoted by the Third Reich?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">Luther</li><li style=\"color:blue\">Luther</li><li style=\"color:blue\">Luther</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Luther</span><br /><span style=\"color:green\"><b>Probability:</b> 42.899%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> The Dalek race, which first appeared in the show's second serial in 1963, are Doctor Who's oldest villains. The Daleks are Kaleds from the planet Skaro, mutated by the scientist Davros and housed in mechanical armour shells for mobility. The actual creatures resemble octopi with large, pronounced brains. Their armour shells have a single eye-stalk, a sink-plunger-like device that serves the purpose of a hand, and a directed-energy weapon. Their main weakness is their eyestalk; attacks upon them using various weapons can blind a Dalek, making it go mad. Their chief role in the series plot, as they frequently remark in their instantly recognisable metallic voices, is to \"exterminate\" all non-Dalek beings. They even attack the Time Lords in the Time War, as shown during the 50th Anniversary of the show. They continue to be a recurring 'monster' within the Doctor Who franchise, their most recent appearances being in the 2015 episodes \"The Witch's Familiar\" and \"Hell Bent\". Davros has also been a recurring figure since his debut in Genesis of the Daleks, although played by several different actors.</span><br /><span><b>Question:</b> Who are the oldest villains from the Doctor Who series? </span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">The Dalek race</li><li style=\"color:blue\">The Dalek race</li><li style=\"color:blue\">The Dalek race</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> the scientist Davros</span><br /><span style=\"color:green\"><b>Probability:</b> 29.857%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> The evolutionary strategy used by cicadas of the genus Magicicada make use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. The logic for this is believed to be that the prime number intervals between emergences make it very difficult for predators to evolve that could specialize as predators on Magicicadas. If Magicicadas appeared at a non-prime number intervals, say every 12 years, then predators appearing every 2, 3, 4, 6, or 12 years would be sure to meet them. Over a 200-year period, average predator populations during hypothetical outbreaks of 14- and 15-year cicadas would be up to 2% higher than during outbreaks of 13- and 17-year cicadas. Though small, this advantage appears to have been enough to drive natural selection in favour of a prime-numbered life-cycle for these insects.</span><br /><span><b>Question:</b> Where do cicadas spend the majority of their lives?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">as grubs underground</li><li style=\"color:blue\">underground</li><li style=\"color:blue\">underground</li><li style=\"color:blue\">underground</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> grubs underground</span><br /><span style=\"color:green\"><b>Probability:</b> 29.714%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Within the Los Angeles Area are the major business districts of Downtown Burbank, Downtown Santa Monica, Downtown Glendale and Downtown Long Beach. Los Angeles itself has many business districts including the Downtown Los Angeles central business district as well as those lining the Wilshire Boulevard Miracle Mile including Century City, Westwood and Warner Center in the San Fernando Valley.</span><br /><span><b>Question:</b> Downtown Burbank is an example of what kind of district?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">business</li><li style=\"color:blue\">major business districts</li><li style=\"color:blue\">major business</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> major business</span><br /><span style=\"color:green\"><b>Probability:</b> 14.629%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> Next, the two plastid-dividing rings, or PD rings form. The inner plastid-dividing ring is located in the inner side of the chloroplast's inner membrane, and is formed first. The outer plastid-dividing ring is found wrapped around the outer chloroplast membrane. It consists of filaments about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast. This is when chloroplast constriction begins.\n",
       "In a few species like Cyanidioschyzon merolæ, chloroplasts have a third plastid-dividing ring located in the chloroplast's intermembrane space.</span><br /><span><b>Question:</b> How many PD rings are there?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">two</li><li style=\"color:blue\">two</li><li style=\"color:blue\">two</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> Next, the two plastid-dividing rings, or PD</span><br /><span style=\"color:green\"><b>Probability:</b> 15.398%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p><span><b>Context:</b> The jewellery collection, containing over 6000 items is one of the finest and most comprehensive collections of jewellery in the world and includes works dating from Ancient Egypt to the present day, as well as jewellery designs on paper. The museum owns pieces by renowned jewelers Cartier, Jean Schlumberger, Peter Carl Fabergé, Hemmerle and Lalique. Other items in the collection include diamond dress ornaments made for Catherine the Great, bracelet clasps once belonging to Marie Antoinette, and the Beauharnais emerald necklace presented by Napoleon to his adopted daughter Hortense de Beauharnais in 1806. The museum also collects international modern jewellery by designers such as Gijs Bakker, Onno Boekhoudt, Peter Chang, Gerda Flockinger, Lucy Sarneel, Dorothea Prühl and Wendy Ramshaw, and African and Asian traditional jewellery. Major bequests include Reverend Chauncy Hare Townshend's collection of 154 gems bequeathed in 1869, Lady Cory's 1951 gift of major diamond jewellery from the 18th and 19th centuries, and jewellery scholar Dame Joan Evans' 1977 gift of more than 800 jewels dating from the Middle Ages to the early 19th century. A new jewellery gallery, funded by William and Judith Bollinger, opened on 24 May 2008.</span><br /><span><b>Question:</b> The earliest items in the jewelry collection come from which ancient civilization?</span><br /><span style=\"color:blue\"><b>Possible answers:</b><br /><ul><li style=\"color:blue\">Ancient Egypt</li><li style=\"color:blue\">Ancient Egypt</li><li style=\"color:blue\">Ancient Egypt</li></ul></span><br /><span style=\"color:green\"><b>Prediction:</b> </span><br /><span style=\"color:green\"><b>Probability:</b> 1.967%</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v7JFJMsaEQAL"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}